{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA4lz-AB3_dL"
      },
      "source": [
        "# **WASTE CLASSIFICATION TRAINING**\n",
        "\n",
        "_*Complete training notebook for ESP32-CAM waste sorter.*_\n",
        "\n",
        "This section is focused on preparing all necessary libraries to support the Tiny CNN Model creation for the Waste Classification Training Dataset, i.e. TensorFlow (v2.10.0), NumPy, MatPlotLib, OpenCV for Python. All necessary data are found in [TrashNet/feyzazkeve](https://www.kaggle.com/datasets/feyzazkefe/trashnet).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL_it0Pp2y5m",
        "outputId": "e604f844-9aa1-4e5e-fa67-b96cfd769a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting ai-edge-litert\n",
            "  Downloading ai_edge_litert-2.1.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting backports.strenum (from ai-edge-litert)\n",
            "  Downloading backports_strenum-1.2.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (25.9.23)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (4.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (5.29.5)\n",
            "Downloading ai_edge_litert-2.1.0-cp312-cp312-manylinux_2_27_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports_strenum-1.2.8-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: backports.strenum, ai-edge-litert\n",
            "Successfully installed ai-edge-litert-2.1.0 backports.strenum-1.2.8\n",
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install numpy matplotlib opencv-python\n",
        "!pip install ai-edge-litert\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import cv2\n",
        "import binascii\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "from google.colab import files\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNsBKQ15DHZ"
      },
      "source": [
        "# **STEP 1: UPLOAD THE DATASET**\n",
        "### Create directory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "gai5u8Xl4N2q",
        "outputId": "71526d80-7c11-4a5c-99d1-2b4c6ad18fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload images to the corresponding folders\n",
            "Required: 320/340 images per class for training, 80/60 for testing\n",
            "Dataset required:\n",
            "\tPlastic, Paper, and Metal.\n",
            "\n",
            "Following the process below, if no need to re-extract the ZIP file for\n",
            "the dataset training and validation, just press the Cancel Upload button\n",
            "and the program will proceed to whatever in the availabe dataset anyway.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2eda81cf-a3ff-4452-9d81-0ad8e1f45d57\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2eda81cf-a3ff-4452-9d81-0ad8e1f45d57\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving IWSS_WasteClassificationDataset.zip to IWSS_WasteClassificationDataset.zip\n",
            "Successfully re-uploaded three dataset_320x80 for `train` and `test`.\n",
            "Successfully re-uploaded three dataset_340x60 for `train` and `test`.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p dataset_320x80/train/plastic dataset_320x80/train/paper dataset_320x80/train/metal\n",
        "!mkdir -p dataset_320x80/test/plastic dataset_320x80/test/paper dataset_320x80/test/metal\n",
        "!mkdir -p dataset_340x60/train/plastic dataset_340x60/train/paper dataset_340x60/train/metal\n",
        "!mkdir -p dataset_340x60/test/plastic dataset_340x60/test/paper dataset_340x60/test/metal\n",
        "\n",
        "print(\"Please upload images to the corresponding folders\")\n",
        "print(\"Required: 320/340 images per class for training, 80/60 for testing\")\n",
        "print(\"Dataset required:\\n\\tPlastic, Paper, and Metal.\\n\")\n",
        "print(\"Following the process below, if no need to re-extract the ZIP file for\")\n",
        "print(\"the dataset training and validation, just press the Cancel Upload button\")\n",
        "print(\"and the program will proceed to whatever in the availabe dataset anyway.\")\n",
        "\n",
        "folders = [\n",
        "    \"dataset_320x80/test/plastic\",\n",
        "    \"dataset_320x80/test/paper\",\n",
        "    \"dataset_320x80/test/metal\",\n",
        "    \"dataset_320x80/train/plastic\",\n",
        "    \"dataset_320x80/train/paper\",\n",
        "    \"dataset_320x80/train/metal\",\n",
        "\n",
        "    \"dataset_340x60/test/plastic\",\n",
        "    \"dataset_340x60/test/paper\",\n",
        "    \"dataset_340x60/test/metal\",\n",
        "    \"dataset_340x60/train/plastic\",\n",
        "    \"dataset_340x60/train/paper\",\n",
        "    \"dataset_340x60/train/metal\"\n",
        "]\n",
        "\n",
        "delete_folders = False\n",
        "reupload_folders = True\n",
        "upload_once = True\n",
        "\n",
        "if delete_folders:\n",
        "    for folder in folders:\n",
        "        if os.path.exists(folder):\n",
        "            for item in os.listdir(folder):\n",
        "                item_path = os.path.join(folder, item)\n",
        "                if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                    os.remove(item_path)\n",
        "                elif os.path.isdir(item_path):\n",
        "                    shutil.rmtree(item_path)\n",
        "            print(f\"Cleared contents of: {folder}\")\n",
        "        else:\n",
        "            print(f\"Folder not found: {folder}\")\n",
        "\n",
        "if reupload_folders:\n",
        "    if upload_once:\n",
        "        uploaded = files.upload()\n",
        "        with zipfile.ZipFile(\"IWSS_WasteClassificationDataset.zip\", \"r\") as zip_ref:\n",
        "            zip_ref.extractall(\"\")\n",
        "\n",
        "    source_base = \"IWSS_WasteClassificationDataset\"\n",
        "    classes = [\"metal\", \"plastic\", \"paper\"]\n",
        "    configs = [\n",
        "        (\"dataset_320x80\", 320, 80),\n",
        "        (\"dataset_340x60\", 340, 60)\n",
        "    ]\n",
        "\n",
        "    for dataset_name, train_count, test_count in configs:\n",
        "        for cls in classes:\n",
        "            src_dir = os.path.join(source_base, cls)\n",
        "            files = sorted(os.listdir(src_dir))\n",
        "\n",
        "            for f in files[:train_count]:\n",
        "                shutil.copy(\n",
        "                    os.path.join(src_dir, f),\n",
        "                    f\"{dataset_name}/train/{cls}/{f}\"\n",
        "                )\n",
        "\n",
        "            for f in files[train_count:train_count + test_count]:\n",
        "                shutil.copy(\n",
        "                    os.path.join(src_dir, f),\n",
        "                    f\"{dataset_name}/test/{cls}/{f}\"\n",
        "                )\n",
        "\n",
        "        print(f\"Successfully re-uploaded three {dataset_name} for `train` and `test`.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMVcUkLoJRlQ"
      },
      "source": [
        "# **STEP 2: LOAD AND PREPARE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv7h7fNyJSye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8b320d-bc9e-489e-8b96-48f784586beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 960 files belonging to 3 classes.\n",
            "Found 240 files belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE_W, IMG_SIZE_H = 240, 320 # Available: [96, 128, 160, 240] | 320x240 for QVGA\n",
        "BATCH_SIZE = 32\n",
        "DATASET_PATH = \"dataset_320x80\" # Another variation: dataset_340x60\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def augment(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = data_augmentation(image, training=True)\n",
        "    return image, label\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomBrightness(0.15),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "])\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    f'{DATASET_PATH}/train',\n",
        "    image_size=(IMG_SIZE_W, IMG_SIZE_H),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='int'\n",
        ")\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    f'{DATASET_PATH}/test',\n",
        "    image_size=(IMG_SIZE_W, IMG_SIZE_H),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "train_ds_aug = train_ds.map(\n",
        "    augment,\n",
        "    num_parallel_calls=AUTOTUNE\n",
        ")\n",
        "\n",
        "train_ds_aug = train_ds_aug.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDA_CrINJsWY"
      },
      "source": [
        "# **STEP 3: CREATE TINY CNN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_zu8t0uJwVo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "c1f4bc5a-0abe-4ec5-c79e-5277c6011dcd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m432\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │            \u001b[38;5;34m64\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m4,608\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │        \u001b[38;5;34m13,824\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │           \u001b[38;5;34m192\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m27,648\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">432</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,608</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,824</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,648</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,347\u001b[0m (184.95 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,347</span> (184.95 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,027\u001b[0m (183.70 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,027</span> (183.70 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m320\u001b[0m (1.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> (1.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Changes:\n",
        "# 1. Removed the Rescaling(1./255) part, so now the model expects raw pixel\n",
        "#    values in [0, 255].\n",
        "# 2. Revamped the models channels into factors of 16 instead of 8 and/or 12.\n",
        "# 3. Added BatchNormalization, removing Bias from Conv layers, and separating\n",
        "#    ReLU from Conv.\n",
        "# 4. Enhancing the Conv2D layering technique, with the final decision of using\n",
        "#    four blocks in order to get better result in the training and validation\n",
        "#    process.\n",
        "\n",
        "# >>> model = tf.keras.Sequential([\n",
        "# >>>     tf.keras.Input(shape=(IMG_SIZE_W, IMG_SIZE_H, 3)),\n",
        "# >>>     tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu'),\n",
        "# >>>     tf.keras.layers.MaxPooling2D(),\n",
        "# >>>     tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "# >>>     tf.keras.layers.MaxPooling2D(),\n",
        "# >>>     tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "# >>>     tf.keras.layers.GlobalAveragePooling2D(),\n",
        "# >>>     tf.keras.layers.Dense(3, activation='softmax')\n",
        "# >>> ])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(IMG_SIZE_W, IMG_SIZE_H, 3)),\n",
        "\n",
        "    # ===== Block 1: Edge & color =====\n",
        "    tf.keras.layers.Conv2D(\n",
        "        16, 3, padding='same', use_bias=False\n",
        "    ),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.9),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    # ===== Block 2: Texture =====\n",
        "    tf.keras.layers.Conv2D(\n",
        "        32, 3, padding='same', use_bias=False\n",
        "    ),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.9),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    # ===== Block 3: Better Abstraction and Class Separation =====\n",
        "    tf.keras.layers.Conv2D(\n",
        "        48, 3, padding='same', use_bias=False\n",
        "    ),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.9),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    # ===== Block 4: Material Patterns =====\n",
        "    tf.keras.layers.Conv2D(\n",
        "        64, 3, padding='same', use_bias=False\n",
        "    ),\n",
        "    tf.keras.layers.BatchNormalization(momentum=0.9),\n",
        "    tf.keras.layers.ReLU(),\n",
        "\n",
        "    # ===== Global aggregation =====\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "    # ===== Classification =====\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ablq2bUFKAnH"
      },
      "source": [
        "# **STEP 4: TRAIN THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp63614lKFwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d02e233-64c6-4488-95b4-fcdf00502e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Safe training is ENABLED!\n",
            "[INFO] EarlyStopping and ReduceLROnPlateau is APPLIED...\n",
            "Epoch 1/60\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 5s/step - accuracy: 0.5113 - loss: 0.9528 - val_accuracy: 0.5167 - val_loss: 0.9316 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 4s/step - accuracy: 0.6666 - loss: 0.7486 - val_accuracy: 0.4958 - val_loss: 0.9578 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4s/step - accuracy: 0.7010 - loss: 0.6960 - val_accuracy: 0.5375 - val_loss: 1.0125 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4s/step - accuracy: 0.7323 - loss: 0.6589 - val_accuracy: 0.6208 - val_loss: 0.8542 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m27/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m12s\u001b[0m 4s/step - accuracy: 0.7334 - loss: 0.6249"
          ]
        }
      ],
      "source": [
        "# Changes:\n",
        "# 1. Added EarlyStopping in the training process for better model output.\n",
        "# 2. Added unsafe_training flag for complete training set without\n",
        "#    EarlyStopping callback, technically unsafe because underfitting\n",
        "#    or overfitting may occur and some losses may happen along the way.\n",
        "# 3. Added ReduceLROnPlateau.\n",
        "\n",
        "# >>> Epochs Configuration <<<\n",
        "# ✅ Best starting/default range: 30–60 epochs\n",
        "# ❌ Avoid:\n",
        "#           < 20 → likely underfitting\n",
        "#           > 80 → likely overfitting (especially without augmentation)\n",
        "\n",
        "# >>> EPOCHS = 40\n",
        "# >>> history = model.fit(\n",
        "# >>>     train_ds,\n",
        "# >>>     validation_data=val_ds,\n",
        "# >>>     epochs=EPOCHS\n",
        "# >>> )\n",
        "\n",
        "safe_training = True\n",
        "\n",
        "# --- TRAINING PROCESS ---\n",
        "# ...For better tuning, set unsafe_training = False.\n",
        "# ...Configure amount of EPOCHS by indexing.\n",
        "EPOCHS = [30, 40, 50, 60][-1]\n",
        "\n",
        "callbacks, history = None, None\n",
        "# class_weight = {\n",
        "#     0: 1.2,  # Plastic\n",
        "#     1: 1.0,  # Paper\n",
        "#     2: 1.3,  # Metal\n",
        "# }\n",
        "\n",
        "if safe_training:\n",
        "    print(\"[INFO] Safe training is ENABLED!\")\n",
        "    print(\"[INFO] EarlyStopping and ReduceLROnPlateau is APPLIED...\")\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=8,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-5\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        train_ds_aug,\n",
        "        # class_weight=class_weight,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "else:\n",
        "    print(\"[INFO] Safe training is DISABLED!\")\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds_aug,\n",
        "        # class_weight=class_weight,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS,\n",
        "    )\n",
        "\n",
        "# --- TRAINING PROCESS ---\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zZ8nIN0KNil"
      },
      "source": [
        "# **STEP 5: CONVERT TO TFLITE FOR ESP32**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Re5YmEqKQkr"
      },
      "outputs": [],
      "source": [
        "# Changes:\n",
        "# 1. Confirming the representative dataset must be float32 in [0, 255].\n",
        "# 2. Removed Rescaling from previous blocks, ensuring everything in raw pixels.\n",
        "# 3. Fixed ds_taken into taking total amount of training samples over batch\n",
        "#    size.\n",
        "# 4. Removed ds_taken logic, just iterate over train_ds directly.\n",
        "\n",
        "# >>> import math\n",
        "# >>> num_samples = 0\n",
        "# >>> for images, labels in train_ds:\n",
        "# >>>     num_samples += images.shape[0]\n",
        "# >>> ds_taken = 150\n",
        "# >>> ds_taken = math.ceil(num_samples / BATCH_SIZE)\n",
        "\n",
        "def representative_dataset():\n",
        "    # >>> for images, _ in train_ds.take(ds_taken):\n",
        "    # >>>    for i in range(BATCH_SIZE):\n",
        "    # >>>        yield [images[i:i+1]]\n",
        "    for images, _ in train_ds:   # NON-augmented dataset\n",
        "        images = tf.cast(images, tf.float32)\n",
        "        for i in range(images.shape[0]):\n",
        "            yield [images[i:i+1]]   # (1, IMG_SIZE_W, IMG_SIZE_H, 3)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('waste_classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"Model saved! Size: {len(tflite_model) / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b5EeUOOKnKR"
      },
      "source": [
        "# **STEP 6: [SAMPLE] TEST THE TFLITE MODEL**\n",
        "\n",
        "### NOTE: Single random sample only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GxPeb-KKp7S"
      },
      "outputs": [],
      "source": [
        "# Changes:\n",
        "# 1. Fixed the Input Preprocess (from ignoring INT8 scaling process before,\n",
        "#    resulting in wrong prediction and such) with the correct INT8 mapping.\n",
        "# 2. Used np.expand_dims instead of list wrapping.\n",
        "# 3. Incorrect output handling produces probabilities instead of INT8 logits,\n",
        "#    thus fixed the output_details part to be more proper in output_float,\n",
        "#    making it is indeed in real-valued logits.\n",
        "# 4. Even though THE model has softmax, INT8 outputs are still quantized.\n",
        "#    Therefore, added changes for the predicted_class and confidence variables\n",
        "#    by introducing new variables exp and probs.\n",
        "# 5. Removed interpreter.get_tensor() logic, can cause confusion and it does\n",
        "#    nothing to the algorithm.\n",
        "# 6. Updated the np.clip() logic inside the INPUT block.\n",
        "\n",
        "interpreter = Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Get one validation sample\n",
        "# --------------------------------------------------\n",
        "for images, labels in val_ds.take(1):\n",
        "    sample_image = images[0].numpy()\n",
        "    sample_label = labels[0].numpy()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "def infer_int8(interpreter, image, input_details, output_details):\n",
        "    # --------------------------------------------------\n",
        "    # INPUT: Quantize float32 -> INT8\n",
        "    # --------------------------------------------------\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "    input_dtype = input_details[0]['dtype']\n",
        "\n",
        "    image_float = image.astype(np.float32)\n",
        "    image_quantized = image_float / input_scale + input_zero_point\n",
        "\n",
        "    info = np.iinfo(input_dtype)\n",
        "    image_int8 = np.clip(\n",
        "        image_quantized, info.min, info.max\n",
        "    ).astype(input_dtype)\n",
        "    # >>> sample_image_int8 = (sample_image * 255).astype(np.int8)\n",
        "\n",
        "    # >>> interpreter.set_tensor(input_details[0]['index'], [sample_image_int8])\n",
        "    interpreter.set_tensor(\n",
        "        input_details[0]['index'],\n",
        "        np.expand_dims(image_int8, axis=0)\n",
        "    )\n",
        "\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # OUTPUT: Dequantize INT8 -> float32 logits\n",
        "    # --------------------------------------------------\n",
        "    output_scale, output_zero_point = output_details[0]['quantization']\n",
        "    output_int8 = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    output_float = output_scale * (\n",
        "        output_int8.astype(np.float32) - output_zero_point\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Softmax (approximate confidence)\n",
        "    # --------------------------------------------------\n",
        "    exp = np.exp(output_float[0] - np.max(output_float[0]))\n",
        "    probs = exp / np.sum(exp)\n",
        "\n",
        "    return probs\n",
        "\n",
        "probs = infer_int8(\n",
        "    interpreter,\n",
        "    sample_image,\n",
        "    input_details,\n",
        "    output_details\n",
        ")\n",
        "\n",
        "# >>> confidence = output[0][predicted_class] * 100\n",
        "predicted_class = np.argmax(probs)\n",
        "# >>> predicted_class = np.argmax(output[0])\n",
        "confidence = probs[predicted_class] * 100\n",
        "\n",
        "class_names = ['Plastic', 'Paper', 'Metal']\n",
        "print(\"Test Result:\")\n",
        "print(f\"True      : {class_names[sample_label]}\")\n",
        "print(f\"Predicted : {class_names[predicted_class]}\")\n",
        "print(f\"Confidence: {confidence:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSJT1qe-FMPc"
      },
      "source": [
        "# **STEP 7: [FULL] TEST THE TFLITE MODEL**\n",
        "\n",
        "### NOTE: Entire waste dataset (Plastic, Paper, and Metal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCu0xtnPFgMh"
      },
      "outputs": [],
      "source": [
        "# ---- STATISTICS ---\n",
        "total = 0\n",
        "correct = 0\n",
        "num_classes = len(class_names)\n",
        "per_class_total = np.zeros(num_classes, dtype=int)\n",
        "per_class_correct = np.zeros(num_classes, dtype=int)\n",
        "\n",
        "# Optional: confusion matrix\n",
        "confusion = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "display_all_samples = True\n",
        "\n",
        "# ---- DATASET LOOP ----\n",
        "for images, labels in val_ds:\n",
        "    images = images.numpy()\n",
        "    labels = labels.numpy()\n",
        "\n",
        "    for i in range(images.shape[0]):\n",
        "        probs = infer_int8(\n",
        "            interpreter,\n",
        "            images[i],\n",
        "            input_details,\n",
        "            output_details\n",
        "        )\n",
        "\n",
        "        predicted_class = np.argmax(probs)\n",
        "        confidence = probs[predicted_class] * 100\n",
        "        true_label = labels[i]\n",
        "\n",
        "        # ---- METRICS ----\n",
        "        total += 1\n",
        "        per_class_total[true_label] += 1\n",
        "        confusion[true_label, predicted_class] += 1\n",
        "\n",
        "        if predicted_class == true_label:\n",
        "            correct += 1\n",
        "            per_class_correct[true_label] += 1\n",
        "\n",
        "        if display_all_samples:\n",
        "            print(f\"[True, Predicted, Confidence]: [{class_names[true_label]}, {class_names[predicted_class]}, {confidence:.1f}%]\")\n",
        "\n",
        "# Print all necessary informations.\n",
        "print(f\"\\nOverall Accuracy: {correct / total * 100:.2f}%\\n\")\n",
        "print(\"Per-Class Accuracy:\")\n",
        "for i, name in enumerate(class_names):\n",
        "    acc = per_class_correct[i] / per_class_total[i] * 100\n",
        "    print(f\"{name:8s}: {acc:.2f}% ({per_class_correct[i]}/{per_class_total[i]})\")\n",
        "\n",
        "# ---- CONFUSION MATRIX GRAPHICAL DISPLAY ----\n",
        "# Additional info: Displaying the Confusion Matrix.\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "im = ax.imshow(confusion, cmap=\"Blues\")\n",
        "\n",
        "# Axis labels\n",
        "ax.set_xticks(np.arange(num_classes))\n",
        "ax.set_yticks(np.arange(num_classes))\n",
        "ax.set_xticklabels(class_names)\n",
        "ax.set_yticklabels(class_names)\n",
        "\n",
        "ax.set_xlabel(\"Predicted Label\")\n",
        "ax.set_ylabel(\"True Label\")\n",
        "ax.set_title(\"Confusion Matrix\")\n",
        "\n",
        "# Rotate x-axis labels\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "# Annotate each cell\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        ax.text(\n",
        "            j, i,\n",
        "            confusion[i, j],\n",
        "            ha=\"center\", va=\"center\",\n",
        "            color=\"white\" if confusion[i, j] > confusion.max() / 2 else \"black\"\n",
        "        )\n",
        "\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvGFKSFZK0Z4"
      },
      "source": [
        "# **STEP 8: CONVERT TO C ARRAY FOR ESP32**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK1ND9iWK4UL"
      },
      "outputs": [],
      "source": [
        "import binascii\n",
        "from google.colab import files # Re-import files to ensure it's the module\n",
        "\n",
        "def tflite_to_c_array(tflite_model, array_name='waste_model'):\n",
        "    hex_data = binascii.hexlify(tflite_model).decode('utf-8')\n",
        "\n",
        "    c_code = f\"// TensorFlow Lite model for waste classification\\n\"\n",
        "    c_code += f\"// Size: {len(tflite_model)} bytes\\n\\n\"\n",
        "    c_code += f\"const unsigned char {array_name}[] = {{\\n    \"\n",
        "\n",
        "    # Format with 16 bytes per line\n",
        "    for i in range(0, len(hex_data), 2):\n",
        "        if i > 0 and i % 32 == 0:\n",
        "            c_code += \"\\n    \"\n",
        "        c_code += f\"0x{hex_data[i:i+2]}, \"\n",
        "\n",
        "    c_code = c_code[:-2] + \"\\n};\\n\\n\"\n",
        "    c_code += f\"const int {array_name}_len = {len(tflite_model)};\"\n",
        "\n",
        "    return c_code\n",
        "\n",
        "c_array_code = tflite_to_c_array(tflite_model)\n",
        "with open('model.h', 'w') as f:\n",
        "    f.write(c_array_code)\n",
        "\n",
        "print(\"\\nC array saved to 'model.h'\")\n",
        "print(\"Download this file for ESP32: files.download('model.h')\")\n",
        "\n",
        "# Download the files\n",
        "download_flagged = True\n",
        "if download_flagged:\n",
        "    files.download(f'waste_classifier.tflite')\n",
        "    files.download(f'model.h')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}